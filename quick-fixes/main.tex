\documentclass[fleqn,usenatbib,twocolumn]{mnras}%twocolumn in square brackets 
\usepackage{newtxtext,newtxmath}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage[normalem]{ulem}
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
%\usepackage{amssymb}	% Extra maths symbols
\usepackage{array, booktabs} % Table
\usepackage{lipsum}
\usepackage{minted}
\usepackage{float}
\usepackage{minted}
%\usepackage{lineno}
\usepackage{url}
%\linenumbers
\date{}
\title[QPOML: Quasi-Periodic Oscillations and Machine Learning]{QPOML: A Machine Learning Approach to Detect and Characterize Quasi-Periodic Oscillations in X-ray Binaries}
% 0000-0002-2363-2487
\author[Kiker et al.]{Thaddaeus J. Kiker,$^{1}$\thanks{E-mail: thaddaeuskiker@protonmail.com}, James F. Steiner$^{2}$,
Cecilia Garraffo$^{2}$, Mariano M\'endez$^3$, and Liang Zhang $^{4}$\\
$^{1}$ Sunny Hills High School, 1801 Lancer Way, Fullerton, CA 92833, USA\\
$^{2}$ Center for Astrophysics | Harvard \& Smithsonian, 60 Garden St. Cambridge, MA 02138, USA\\
$^{3}$ Kapteyn Astronomical Institute, University of Groningen, P.O. BOX 800, 9700 AV Groningen, The Netherlands\\
$^{4}$ Key Laboratory for Particle Astrophysics, Institute of High Energy Physics, Chinese Academy of Sciences, Beijing 100049, People's Republic of China
} 
 
\date{Accepted XXX. Received YYY; in original form 2022 November 9}

\pubyear{2022}

\begin{document}
\label{firstpage}  
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

\begin{abstract}
Astronomy is presently experiencing profound growth in the deployment of machine learning to explore large datasets. However, transient quasi-periodic oscillations (QPOs) which appear in power density spectra of many X-ray binary system observations are an intriguing phenomena heretofore not explored with machine learning. In light of this, we propose and experiment with novel methodologies for predicting the presence and properties of QPOs to make the first ever detections and characterizations of QPOs with machine learning models. We base our findings on raw energy spectra and processed features derived from energy spectra using an abundance of data from the \textit{NICER} and \textit{Rossi X-ray Timing Explorer} space telescope archives for two black hole low mass X-ray binary sources, GRS 1915+105 and MAXI J1535-571. We advance these non-traditional methods as a foundation for using machine learning to discover global inter-object generalizations between—and provide unique insights about—energy and timing phenomena to assist with the ongoing challenge of unambiguously understanding the nature and origin of QPOs. Additionally, we have developed a publicly available Python machine learning library, QPOML, to enable further Machine Learning aided investigations into QPOs.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
\begin{keywords}
accretion, accretion disks --- black hole physics --- stars: individual (GRS\, 1915+105,\, MAXI\, J1535+571) --- X-rays: binaries
\end{keywords}

\section{Introduction}

At the ends of their lives, massive stars ``do not go gentle into that good night'' \citep{thomas1952country}. Instead, if their initial mass exceeds $\sim8$ M$_\odot$, core collapse leads to spectacular Type II supernovae \citep{Schlegel1995}. If the compact remnant remains bound or becomes bound to a non-degenerate companion star, the result can be a neutron star (NS) or black hole (BH) remnant \citep{gilmore2004}. In special cases, this object maintains a non-degenerate partner, and together these may form an X-ray binary (XRB) system, in which the non-degenerate star engages in mass-exchange with its compact partner \citep{tauris2006}. Such systems are characterized by accretion from the donor star, through accretion disks \citep{SS73} and are the sources for jets \citep{gallo2005,neutronstarjet} and winds \citep{bhwinds,neutronStarWind}. Additional exotic phenomena like thermonuclear surface burning  \citep{thermonuclear} have also been observed in neutron star binaries. Both BH and NS systems are both observed to emit thermal X-ray radiation with temperatures $\sim1$ keV that is understood to arise from the conversion of gravitational potential to radiative energy. Neutron stars can produce thermal emission at their surfaces, and the optically thick, geometrically thin accretion disks around both NSs and BHs can produce strong thermal X-ray emission \citep{SS73}. Furthermore, BH and NS XRBs both also show hard X-ray flux coming from Compton up-scattering of thermal disk emission by a cloud of hot electrons around the compact source known as the corona \citep{corona1979,coronae1982}. Comptonized emission is commonly modeled by a power law relationship $N(E)\propto E^{-\Gamma}$, where $\Gamma$ is the photon index \citep{McClintockRemillard2006}. Strongly-Comptonized spectra commonly exhibit reflection features like a fluorescent, relativistically broadened 6.4 keV Fe K$\alpha$ line \citep{ironlines1989} and $\sim30$ keV Compton hump \citep{x-rayreflectionmodels2005}. These systems can be transient in activity and undergo evolution in spectral states \citep{gardenier2018}, ranging from hard, to intermediate, and to soft \citep{McClintockRemillard2006}, which are coupled with mass-accretion rate \citep{terrabytedone}, spectral hardness or thermal dominance, and thereby position on a hardness-intensity or color-color diagram track \citep{ingram2019}, and the presence/absence of quasi-periodic oscillations (QPO) of the observed X-ray radiation \citep{McClintockRemillard2006}. These QPOs are detected as narrow peaks in power-density spectra \citep{homanBelloniQPOstates}. In the past thirty years, numerous theories, including but not limited to relativistic precession \citep{lense-thirring-original}, precesssing inner flow \citep{precessionandlense}, corrugation modes \citep{corrugation}, accretion ejection instability \citep{accretion-ejection}, and propagating oscillatory shock \citep{propagatingoscillatoryshock} have been advanced to explain the occurrence of QPOs in black hole, as well as neutron star, XRB systems. Yet, there is not consensus as to which model is most plausible. In black-hole systems, most of the observed QPOs have been at low frequencies (LF) $\leq 30$ Hz \citep{belloni2020typeB}. Only a small subset has BHXRBs have exhibited high-frequency QPOs (HFQPO). LF QPOs are further subdivided canonically into three classes \citep{casellaABC}: Type-A QPOs are the rarest, sometimes appearing in the intermediate or soft state as broad, low amplitude features centered between 6-9 Hz and usually lacking harmonic companions \citep{motta2011}. Type-B QPOs are more common, and can be seen during the short soft intermediate state and have shown some connection with jet behavior \citep{globaltypeB,garciaMendez2021}. Finally, type C QPOs are the most common, and can be detected  as narrow features in the low-hard and hard-intermediate states with harmonic companions  \citep{typeCmodel2016}. Their fundamental frequencies range from $\sim$ 0.1-30 Hz depending on state, and almost always correlate strongly with spectral features like $\Gamma$ and luminosity \citep{motta2015LotsofQPOs}. As for HFQPOs, we recommend readers to \cite{motta2011}, \cite{MendezHFQPOs2013}, and \cite{stellaVietriISCO}. QPOs are also observed in neutron star systems \citep{Belloni2002,wang2016QPOreview}. We focus on LFQPOS from BHXRBs in this paper and recommend \cite{vanDerKlis2006} and \cite{wang2016QPOreview} for reviews of neutron star specific QPOs and \cite{ingram2019}, \cite{oneHZqpo}, \cite{hectohertzKato2005}, \cite{Revnivtsev2001}, and \cite{mendezBelloni2021} of QPOs in XRBs in general. All in all, hundreds of XRBs have been observed since the discovery of Sco X-1 \citep{scox-1,lmxbCatalog,blackcat} and a large fraction show some type of QPO. 

Machine learning is a revolutionary subfield of artificial intelligence in which models teach themselves patterns in data rather than operating by externally supplied hard-coded rules \citep{goodfellow2016deep}. With data available to astronomers approaching the petabyte domain \citep{2014MLinAstro}, this aspect of machine learning has helped it supplement traditional methods in addressing the ever growing volume and increasing complexity of astronomical data, while also providing new perspectives on old phenomena \citep{bigUNIbigDATA,astroMLarticlenotPython}. Consequently, machine learning has been used prolifically to classify variable stars \citep{mlVariableStars2011}, search for exoplanets \citep{mlExos}, detect pulsars \citep{detectPulsars}, predict solar flares \citep{solarflareML}, classify and even discover galaxies \citep{classifyGalaxies,discoverGalaxies}. However, although machine learning techniques has been applied to a number of problems related XRBs as well, e.g, to classify and identify X-ray binaries \citep{grsML2017,arnason2020,Sreehari2021,deBeurs2022,grsML2022,yang2022}, predict compact object identity \citep{Pattnaik2021}, and study gravitational waves \citep{MLgravWave}, this subfield contains tens of thousands of observations that have never been explored with machine learning to detect QPOs themselves. For the first time, in this work we seek to develop a methodology for using machine learning to detect QPOs, because we believe that our theoretical understanding of QPOs and their exotic progenitor systems would benefit from insights this approach could provide \citep{mlandtheory}. Our approach is unique, because although the externally determined presence of QPOs has been used as a binary input parameter in accretion state classifiers such as those in \cite{Sreehari2021}, QPOs have never before been the output of machine learning prediction themselves. The rest of this paper is structured as follows: in Section \ref{sec:obs} we describe the observations upon which we base our work. Following this, in Section \ref{sec:data_analysis} we describe the energy and spectral fitting procedures we employ to produce input/output data from these observations for the machine learning models and methods which we detail in Section \ref{sec:methods}. We present our results in Section \ref{sec:results}, and we discuss these results contextually in Section \ref{sec:discussion}. Finally, we conclude in Section \ref{sec:conclusion}. Additional work concerning demonstrating \texttt{QPOML} and model comparison are presented in following appendices. 

\section{Observations}\label{sec:obs}

%\begin{table}
%    \centering\label{tab:sources}
%    \caption{Description of Sources %Included in this Study}
%%    \begin{tabular}{l c c c}
%    \hline 
%    \hline
%    Source & Class & Instrument & Number %of Observations  \\
%    \hline
%    MAXI J1535-571 & BH & \textbf{\textit{NICER}} & 270 () \\
%    GRS 1915+105 & BH & \textbf{\textit{RXTE}} & 620 \\
%   \hline
%    Total & & & 1278 \\
%    \hline 
%    \end{tabular}
%\end{table}

\subsection{GRS 1915+105}
% https://academic.oup.com/mnras/article/349/2/393/956946

GRS 1915+105 is a well studied galactic low mass XRB system composed of a $12.4^{+2.0}_{-1.8}$ M$_{\odot}$ primary and a $1.2$ M$_{\odot}$ K III secondary \citep{GRSCompanion,Greiner2003} on a $34$ d period located at a distance of $8.6^{+2.0}_{-1.6}$ kpc from the Earth \citep{GRSDISTANCE}. The secondary star in this system overflows its Roche lobe. GRS 1915+105 was one of the first microquasar jet systems, with (apparent) superluminal motion detected from a ballistic jet launched with an inclination $70\pm2\deg$ \citep{grsjetinclination}. Since its discovery in 1992 \citep{grs-discovery}, this somewhat peculiar source has displayed unique timing and spectral patterns which have been organized into 14 separate variability classifications depending on its variability state \citep{belloni2000,Hannikainen2005}. With its 16-year archive of observations of this source we considered all data from the Rossi X-ray Timing Explorer (RXTE) Proportional Counter Array (PCA; $2-60$ keV) that are also included in \cite{GRSDATAPAPER}, \cite{mendez2022couplingNATURE}, and \cite{garciaGRS2022MNRAS}. These include a great number of detections of type C QPOs between 1996 and 2012. Energy and power-density spectra (PDS) have been derived from binned, event, and GoodXenon data as described in \cite{GRSDATAPAPER}. Briefly, PDS have been constructed by averaging 128 s long intervals at 1/128 s time resolution, normalized according to \cite{leahy1983norm}, and Poisson noise subtracted \citep{zhangPDSsubtraction}. Of the 625 timing observations in \cite{GRSDATAPAPER}, we have 554 matching energy spectra.  

\subsection{MAXI J1535-571}

MAXI J1535-571 was discovered by the MAXI/GSC nova alert system as a hard X-ray transient system undergoing outburst in 2017 by \cite{maxiATELdiscovery}, and it was first suggested to be black hole system by \cite{MAXIisBHATel}. Since discovery, it has been suggested as a $\sim 10.39M_{\odot}$ BH, $\sim 5$ kpc distant \citep{MAXI-distance}. MAXI J1535-571 has displayed state transitions \citep{Nakahira2018}, reflaring events \citep{cuneo2020}, and hysteresis during its main outburst \citep{hysteresis-MAXI}. Furthermore, it has been determined to possess a near-maximal dimensionless spin parameter of $a = \frac{cJ}{GM^2}>0.99$ \citep{miller2018,HXMT-MAXI-SPIN}. To study this source we use data from the International Space Station mounted, soft X-ray (0.5-12 keV) observatory Neutron star Interior Composition ExploreR (NICER) \citep{NICER} which has unequaled spectral-timing capabilities in soft X-rays.  

We have filtered our \textit{NICER} data following standard practices, excluding South Atlantic Anomaly passages in order to identify continuous good time intervals (GTIs) which are extracted and analyzed individually.  Data from detectors 14, 34, and 54 have been excised owing to a propensity for elevated noise or spurious events in those detectors.  Additionally, for each GTI, the average event rates of overshoot, undershoot, and X-ray events are compared amongst the detector ensemble, and any detector which has a median absolute deviation (MAD) $>15$ is also excised for that GTI\footnote{The MAD is a robust statistic which is insensitive to outliers. 15 MAD corresponds to approximately 10$\sigma$ for a Gaussian-distribution.}).  All spectra have been corrected for deadtime (generally $<1\%$). {\em NICER} backgrounds have been computed using the 3C50 background model \citep{Remillard_3C50}, as well as using a proprietary and similar background model which replaces the 3C50's ``hrej'' and ``ibg'' indexing with cutoff-rigidity ``COR\_Sax'' and overshoot-rate indexing. We have removed any data with a background count rate $\geq5$ counts/s, exclude observations for which the source-to-background count ratio is $<10$, and reject observations with exposure times $t\simeq60$ s. Additionally, we require at least 5000 net source counts to ensure reliable energy and power-density spectral results, and we consider the remaining data sufficiently bright and insensitive to the selection between these similar background models. Energy spectra have been rebinned from the 10 eV PI channels by a factor ranging from 2--6 in order to oversample {\em NICER}'s energy resolution by a factor $\gtrsim 2$, while also requiring a minimum of 5 counts per bin. From $1-4096$ Hz, PDS are computed using events in the energy range from $0.2-12$ keV, for a light-curve sampling at 2$^{-13}$s ($\approx 122\mu$s). PDS are computed individually and averaged together using 4s segments for $t<160$s and 16s segments for $t\geq160$s. Below 1 Hz, PDS are computed by averaging together results for 128s segments for $t\geq 128s$ 64s segments for $64\leq t<128s$ and 4s segments for $t<64s$.  The resulting PDS is then logarithmically rebinned in  $\sim3$\% frequency intervals, the Poisson noise subtracted, and the rms$^2$ Hz$^{-1}$ normalization adopted.

Although we have less MAXI J1535-571 observations with QPOs for analysis (in large part due to the source's transient nature), one benefit of using \textbf{\textit{NICER}} over \textbf{\textit{RXTE}} data for this source (if we could have used \textbf{\textit{RXTE}} data) is that \textbf{\textit{NICER}} spectral channels do not suffer from gain drift over epochs like \textbf{\textit{RXTE}} PCA (which affected energy-channel conversions), and thus we can use the \textbf{\textit{NICER}} energy spectra as raw inputs to our regression and classifier models, in addition to the engineered features discussed in Section \ref{sec:data_analysis} and Section \ref{sec:feature_engineering}.

Overall, we selected these two sources for this initial evaluation of our methodology because they represent two very different types of LMXRBs. On one hand, GRS 1915+105 has long been known as a markedly unusual source in terms of its outburst behaviors and states (e.g. its very abnormal, three-decade long transient outburst, regular/irregular bursts, dips, etc., behaviors influenced by GRS 1915+105's orbital period and accretion disk size, the longest and largest respectively known among LMXRBs), wheres on the other hand, MAXI J1535-571 is, in comparison to GRS 1915+105, a far more typical source in terms of outburst states, QPO-spectral parameter associations, and tracks through the hardness-intensity diagram \citep{Taam1996,Truss2006,Nakahira2018,Bhargava2019,cuneo2020,Koljonen2021,garciaGRS2022MNRAS}. Hence, between these two sources we aim to evaluate our methods across a spectrum of typical to challenging spectral-timing relationships. Furthermore, in choosing objects observed with different instruments, we aim to take advantage of the different strengths of each instrument, such as \textbf{\textit{RXTE'}}s plethora of observations and \textbf{\textit{NICER'}}s high spectral resolution \citep{NICER}.

% Originally focused on providing rotation-resolved spectroscopy of neutron stars with unprecedented sensitivity, \textbf{\textit{NICER}} has also been utilized extensively to study black holes in XRBs, e.g. \cite{cuneo2020} \textbf{add more}

\begin{figure*}
    \centering
\includegraphicswidth=0.48\textwidth{figures/figure_1/GRS_lightcurve.pdf}
\includegraphicswidth=0.48\textwidth{figures/figure_1/MAXI_lightcurve.pdf}
    \caption{Light curves of GRS 1915+105 (left) and MAXI J1535-571 (right) for the observations used in this work. Net count rates are calculated as the sum of the background subtracted counts divided by observation time for every observation of each source. Note the persistent nature of GRS 1915+105 versus the transient flare of MAXI J1535-571 (reflaring epochs of MAXI J1535-571 are not included given the lack of QPOs detected there in previous works).}
    \label{fig:lightcurve}
\end{figure*}

\begin{figure*}
    \centering

    % MAXI % 
\includegraphicswidth=0.24\textwidth{figures/figure_2/1050360105_21spectrumdata.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_2/1050360105_21spectrummodel.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_2/1050360105_21pdsdata.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_2/1050360105_21pdsmodel.pdf}
    
    % GRS %
\includegraphicswidth=0.24\textwidth{figures/figure_2/40116010107spectrumdata.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_2/40116010107spectrummodel.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_2/40116010107pdsdata.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_2/40116010107pdsmodel.pdf}
    
    \caption{Example energy and power density spectra and models for MAXI J1535 observation 1050360105-21 on the top and the same for GRS 1915+105 observation 40116-01-01-07 on the bottom. For each row, from left to right, the first plot shows the energy spectrum and folded \texttt{tbabs*(nthcomp+diskbb)} model, the second shows energy spectrum model alone, the third shows the power density spectrum in the relevant frequency range, and the fourth shows the best fit Lorentzian PDS model alone. Best fit QPO features have been superimposed over zero centered Lorentzians used to model the power-density continuum. Only the fundamental (i.e. first harmonic) is fit for the GRS 1915+105 QPO (as discussed in Section \ref{sec:data_analysis}, this was an intentional choice to see how the models fair with seemingly simpler outputs).} 
    \label{fig:HID}
\end{figure*}


%\begin{figure*}
%    \centering
%\includegraphicswidth=0.4\textwidth{figures/figure_3/GRShardnessvsfrequency.pdf}
%\includegraphicswidth=0.42\textwidth{figures/figure_3/GRShardnessvsnetcountrate.pdf}
%    \caption{QPO fundamental frequency as a function of hardness ratio (defined in Section \ref{sec:data_analysis}) and net count rate as well as net count rate as %a function of QPO fundamental frequency and hardness ratio for observations of the black hole source GRS 1915+105 (see also \cite{evolvingPropertiesGRSCorona}). %The relative  degeneracies between parameters in these plots demonstrate the benefits from making predictions based on higher dimensional input spaces.}
%    \label{fig:HID}
%\end{figure*}

\section{Data Analysis}\label{sec:data_analysis}

\subsection{Energy Spectra}

%\textbf{Notes}
%\begin{itemize}
%    \item Both had disk temperature freely ranging from 0.2 to 3.0 keV, with nthcomp gamma %from 1.1 to soft upper limit of 3.5 and hard upper limit of 4.0. They both had kTin set to %diskbb disk temp. however, they are different in the ranges that their high temperature %rollover was allowed to traverse: for MAXI it was allowed to traverse 4-250 keV, whereas %for GRS it was allowed to traverse 4-40 keV (there are papers I based this off of, but %ignore why I didn't include it. not worth it)
%\end{itemize}

As previously mentioned and discussed in more detail in Section \ref{sec:feature_engineering}, we base our detection of QPOs on energy spectra and processed features from the energy spectra. Thus, to generate the processed spectral features we fit the energy spectra for both sources with \texttt{XSPEC} version $12.12.0$ \citep{XSPEC1999} using the three component model \texttt{tbabs*(diskbb+nthcomp)}, which represents a Tuebingen-Boulder absorbed multi-temperature blackbody and thermally Comptonized continuum \citep{Mitsuda1984,Zdziarski1996,Kubota1998,Zycki1999}. We fixed the equivalent hydrogen column densities to canonical values of $6\times10^{22}\;\mathrm{atoms}\;\mathrm{cm}^{-2}$ for GRS 1915+105 and $3.2\times10^{22}\;\mathrm{atoms}\;\mathrm{cm}^{-2}$ for MAXI J1535-571 based on \cite{astrosatviewofGRS} and \cite{cuneo2020}, respectively, with solar abundances in accordance with \cite{Wilms2000} and \cite{VernerCrossSections} cross-sections. We tied the \texttt{nthcomp} seed photon temperature to $\mathrm{T}_{\mathrm{in}}$ of \texttt{diskbb} for both sources, and let high energy rollover (electron temperature) freely vary between $4-40$ keV for GRS 1915+105 and $4-250$ keV during fitting for MAXI J1535-571, basing these ranges on \cite{zhangGRS2022} and \cite{Dong2022MAXIkTe}, respectively. For GRS 1915+105, we ignore channels $<2.5$ keV or $>25$ keV during fitting, calculate net count rate from the resulting range, and compute hardness as the sum of the ratio of the background subtracted channel net count rates for the ranges in \cite{zhangGRS2022}, except as a proportion rather than a ratio, i.e. $\frac{[13-60]\;\mathrm{keV}}{[2-7]+[13-60]\;\mathrm{keV}}$. Regarding MAXI J1535-571, we note the presence of instrumental residuals in the $1.7-2.3$ keV \textit{NICER} range, likely related to \textit{NICER}'s Au mirror coating and residual in the Si K $\alpha$ fluorescence peak, and following \cite{miller2018}, we address these by excluding the $1.7-2.3$ keV energy band from the spectral fitting process, and otherwise fit the range $0.5-10.0$ keV. We compute net count rate normalized to the number of \textbf{\textit{NICER}} detectors, and hardness ratios for MAXI J1535-571 observations as the proportion of the total net count rate contributed by the $3.0-10.0$ keV range, i.e. $\frac{[3.0-10.0]\;\mathrm{keV}}{[0.5-1.7]+[2.3-3.0]+[3.0-10.0]\;\mathrm{keV}}$. Altogether, for both sources we use the net count rate, hardness ratio, asymptotic power-law photon index, \texttt{nthcomp} normalization, inner disk temperature, and \texttt{diskbb} normalization for input parameters, which we discuss in more detail in Section \ref{sec:feature_engineering}. 

\subsection{Power Density Spectra}

Throughout this work, all QPOs for both sources are parameterized as Lorentzian distributions given by Equation \ref{eq:lorentz},

\begin{equation}
    A(f)=\frac{K(\frac{\sigma}{2\pi})}{(f-f_0)^2+(\frac{\sigma}{2})^2}
    \label{eq:lorentz}
\end{equation}

\noindent where $f$ is frequency in Hertz, $\sigma$ is full width at half maximum (FWHM), and $K$ is the normalization, as per \cite{XSPEC1999}. In the case of GRS 1915+105, QPO properties are obtained by fits to PDS following \cite{GRSDATAPAPER}. A QPO is considered significant when the ratio of the QPO power integral divided by its $1\sigma$ error $>3$ or quality factor $Q=\frac{v_0}{\sigma}$) $>2$ \citep{quality1999}, provided their frequency does not change significantly in an observation. Our primary use for this GRS 1915+105 data is to train machine learning regression models to predict the properties of the fundamental QPO feature, since \textit{only} data with matching QPO detections are used in our GRS 1915+105 machine-learning analysis. In all, this corresponds to $554$ QPOs. In contrast to this approach of fitting individual QPOs solely for regression, we use the energy and timing data from MAXI J1535-571 to explore both classification of observations into binary states of QPO presence/absence as well as multiclass QPO cardinality states \footnote{Also called multinomial classification \citep{bouveyron2019model}, when number of classes totals to $\geq 3$} based on binned raw energy spectra and processed features. Additionally, for MAXI J1535-571 we predict the properties for both the fundamental and frequently appearing harmonic in the PDS based on binned energy spectra and spectral parameterizations derived from energy spectra. Our QPO detection method for MAXI J1535-571 is slightly different than that of GRS 1915+105. Specifically, we determine the presence and properties of QPOs in PDS from MAXI J1535-571 by first fitting two zero-centered Lorentzian functions to PDS and then iteratively fitting a third Lorentzian over a logarithmically sampled set of $268$ frequencies $f$  between $1$ and $20$ Hz, where width is kept $\sigma<\frac{f}{10}$ for an initial fit, and then freed for a subsequent refined fitting step. A peak of qualifying distance ($\Delta \chi^2$ distance to neighboring samples) and threshold (horizontal distance between samples) is identified with the \texttt{scipy} function \texttt{find\_peaks} \citep{scikit-learn} in the resulting distribution of $-1\cdot\chi^2$ fit-statistic with peak height greater than the $\Delta10$ Akaike Information Criterion \citep{Akaike1998}. Finally, a visual inspection is required to accept a QPO candidate detection (to avoid potential spurious detections, e.g., at the frequency boundary). In $68$ of observations the fundamental is accompanied by the second harmonic (the fundamental itself is called the first harmonic), in $14$ observations it is alone, and in $188$ observations no QPO is detected. 

\section{Machine Learning Methods}\label{sec:methods}

\subsection{Model Selection}

In machine learning, models can be broadly divided by two sets of classification: (i) whether they operate in a supervised or unsupervised manner; and (ii) whether they are built for classification or regression \citep{bruce2017practical}. Since we are providing our models with explicit targets for loss minimization, our approach falls under the umbrella of supervised learning \citep{supervisedreview}, and as we are attempting to connect spectral information about XRBs with real-valued output vectors that describe QPOs in their power-density spectra, we also fall under (multi-output) regression \citep{multioutputreview}. In selecting our machine learning models for regression, we seek those that natively support multi-output regression, incorporate capabilities for mitigating overfitting, have precedents of working successfully with medium to small sized data sets, and natively communicate feature importances. Additionally, we seek to evaluate a collection of models against each other in light of the No-Free-Lunch-Theorem \citep{NoFreeLunch,avoidMLpitfalls}.  
    
Based on these criteria, we settle on a set of tree-based models and their descendants, specifically decision trees \citep{breiman1984original}, random forests \citep{breiman2001random}, and Extremely randomized trees \citep{extratrees}. Here we provide a brief summary of these models for context. Decision trees are the original tree-based regression model which operate by inferring discriminative splits in data and making predictions via a series of ``if-then-else'' decisions \citep{breiman1984original}. Random forests are more powerful derivatives of decision trees, and are based on an ensemble of decision trees trained via bootstrap aggregation \citep{breiman1996bagging,breiman2001random}. By incorporating predictions from such an ensemble, random forests reduce prediction variance while increasing overall accuracy when compared to a single decision tree \citep{lakshminarayanan2016decision}. Finally, Extremely randomized trees (also known as extra trees) are similar to random forests in this respect but operate with more randomization during the training process, as instead of employing the most discriminative thresholds within feature spaces for splits, extra trees select the best performing randomly drawn thresholds for splitting rules \citep{extratrees,scikit-learn}. Details on training and optimization are given in Section \ref{subsec:train_validate_tune}, where we also discuss our steps to avoid overfitting \citep{bruce2017practical}.

Together, these represent some of the most powerful yet lightweight machine learning models available, and meet our criteria for multi-output regression \citep{multioutputreview}, robustness to overfitting \citep{metarandomforests,evalTree-BasedEnsembles}, success with small/medium sized datasets \citep{floares2017smallest}, and feature importances \citep{featureimportancestrees}. An additional benefit of these models is that they are natively supported by the \texttt{TreeExplainer} method in the \texttt{SHAP} Python package \citep{SHAP2017}, which frees us from common pitfalls related to impurity and permutation based feature importances, which we discuss in more detail in Section \ref{sec:discussion}. Overall, we explore all the above models in addition to ordinary linear regression (to provide a base performance comparison) for the regression cases, but focus on random forest and logistic regression \citep{original-logistic} for classification cases.  

\subsection{Feature Engineering}\label{sec:feature_engineering}

As \cite{casari2018feature} detail, feature engineering is the process of transforming raw data to maximize predictive performance. After experimenting with different formats, we settled on the following in order to use derived features from spectral fits or raw spectral data as predictors and timing features as outcomes. We will hereafter refer to and experiment with two types of input data for our models: the first are rebinned net energy spectra, which we discuss below and will simply call ``energy spectra.'' The second type is the combination of \texttt{XSPEC} model-fit parameters and spectrum derived features like net count rate and hardness which we will designate the ``engineered features'' input type. When using engineered features for inputs, we format our input data as a matrix composed of vectors containing the net count rate, hardness ratio, asymptotic power-law photon index, \texttt{nthcomp} normalization, inner-disk temperature, and \texttt{diskbb} normalization for every observation. Hereafter, we refer to and present these values by the letters $\{A,B,C,D,E,F,G\}$ as shorthand. This input structure is visualized in Equation \ref{mat:context_matrix} as follows,

\begin{equation}\label{mat:context_matrix}
    \mathrm{IN}_{m\times7} =
    \begin{bmatrix}
    A_1 & B_1 & C_1 & D_1 & E_1 & F_1 & G_1 \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
    A_m & B_m & C_m & D_m & E_m & F_m & G_m \\
    \end{bmatrix}
\end{equation}

\noindent where $m$ is the number of observations. This format can be extended to any $n-$dimensional number of features, which we take advantage of when using raw energy spectra as input data. For the case of MAXI J1535-571, we compare the predictive performance of the models and provide different insights by using raw spectral data in the form of count rate values from $19$ channels, 0.5 keV wide apiece spanning the energy range $[0.5-10.0]$ directly as the input vectors within the input matrix, similar to \cite{Pattnaik2020}. This coarse spectral input strikes a balance between sparsity and precision, allowing us to determine importances for specific $0.5$ keV ranges while not overwhelming the models with too many input features given the overall sample size \citep{smallsample1991,van2020small}. With regards to regression, our QPO output matrix is similarly formatted as a vector matrix, with rows that match by index to vectors in the input matrix, but with an important addition regarding ordering (detailed below). A significant challenge relates to the prediction of not only the presence versus absence of QPOs in a given PDS, as well as (for present cases) the specific number of QPOs and the physical parameters of each QPO present. Over the course of an outburst, the number of QPOs present can change, as these are transient phenomena \citep{Remillard2006,ingram2019}. We account for this challenge of variable output cardinality by first identifying all QPO occurrences associated with an observation. Then, we order these occurrences and their features in a vector of length $L=N_f\times \mathrm{max}(N_s)$, where $N_f$ is the number of features describing every QPO (e.g. $N_f=3$ for frequency, width, and amplitude), and $N_s$ is the maximum number of simultaneous QPOs observed in any particular PDS in a data set. We then structure each output vector as a repeating subset of features for every QPO contained, and order these internal QPO parameterizations by frequency. If one or more of these occurrences are not detected in a PDS, their feature spaces in the vector are populated with zeros. This allows us to circumvent the aforementioned difficulty with variable output cardinality, because the models will learn during training to associate indices populated with zeros as QPO non-detections \citep{deepLearningPython}. As with input features, Equation \ref{mat:qpo_matrix} provides a visualization of the general QPO matrix output returned by our model, where each row corresponds to one observation matched with a row in the input matrix (both out of $m$ total observations). 

\begin{equation}\label{mat:qpo_matrix}
    \mathrm{OUT}_{m\times n} =
    \begin{bmatrix}
    f_{1,1} & \sigma_{1,1} &  K_{1,1} & \hdots & f_{1,n} &  \sigma_{1,n} &  K_{1,n}\\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
    f_{n,1} & \sigma_{m,1} & K_{m,1} & \hdots & f_{m,n} & \sigma_{m,n} & K_{m,n} \\
    \end{bmatrix}
\end{equation}

In the case of MAXI J1535-571, the maximum number of QPOs simultaneously observed in a PDS is two, and each QPO is described in terms of its frequency, width, and amplitude, so the output matrix takes the shape $\mathrm{OUT}=m\times6$. Since we only regress for the fundamental in the GRS 1915+105 PDS, its output matrix takes the form $\mathrm{OUT}=m\times3$. Prior to reformatting the data in this manner, we applied a columnar min-max standardization to the \texttt{XSPEC}, and hardness input features, as well as the QPO Lorentzian output features, which linearly transformed each distribution into a $[\mathrm{max}(x'),\mathrm{min}(x')]=[0.1,1]$ range (as opposed to the traditional $[0-1]$ range given our decision to denote QPO non-detections with zero values) while preserving their shapes, according to Equation \ref{eq:minMax} \citep{Kandanaarachchi2019}.

\begin{equation}
    x' = \frac{x-\mathrm{min}(x)}{\mathrm{max}(x)-\mathrm{min}(x)} \times \frac{\mathrm{max}(x')-\mathrm{min}(x')}{\mathrm{min}(x')} \label{eq:minMax}
\end{equation}

This step is necessary to prevent features with relatively larger absolute amplitudes receiving undue weight, and it also frees the models from dependency on measurement units \citep{Akanbi2015, Han2012}. We did not apply this standardization step to channel count and net count rate input features, however, as the imposition of \textit{a priori} theoretical limits to these features is not as readily justifiable \citep{Pattnaik2020}.  \footnote{Standardization prior to splitting data into train and validation sets does not impair our model's predictive validity when input features are derived from 
\texttt{XSPEC} because its pre-adjusted inputs will always be constrained within the theoretical bounds applied during standardization for each feature (e.g. $\Gamma$ will always initially range between $x-y$ for a source, where $x$ can be a hard lower limit like $\Gamma=1.1$ and $y$ can be the corresponding hard upper limit during fitting, such as $\Gamma=5$).}

\subsection{Training, Validation, and Hyperparameter Tuning}\label{subsec:train_validate_tune}

To better understand our models in different data combinations and minimize statistical noise, while guaranteeing every observation gets included in a training, as well as at a separate time, validation instance, we employ a repeated $k$-fold cross-validation strategy \citep{olson2008advanced,repeatedkfold} for model evaluation (as opposed to solely using a default proportion-based train-test split). According to this procedure, our data is first split into a 90\% training and validation set, and then a 10\% held out test set. Before evaluating the models on this test set, the training and validation set is randomly split into $k=10$ folds. Given the relative class imbalance in the MAXI J1535-571 data in favor of observations without QPOs, for MAXI J1535-571, the folds for both regression and classification cases are also stratified during splitting, which means each fold maintains the same proportion of observations with QPOs \citep{ma2013imbalanced}. Then, every model is evaluated on each unique fold after being trained on the remaining folds, with the individual $k$-fold performance taken as the mean of these evaluations across the ten folds. We repeat this process five times (randomly shuffling the data between each iteration), and the final score for each model is calculated as the mean performance across the ten $k$-fold instances, either as the $f-$score for classification cases (a harmonic mean of the precision and recall), or the median absolute error for regression \citep{scikit-learn,kuhn2019applied}. Random initialization is kept the same between models to make sure each model is trained/tested on the same data within each fold, and to ensure fair comparison between these models, each was subject to automatic and individualized hyperparameter tuning via grid search prior during this evaluation \citep{dangeti2017statisticsML}. The specific hyperparameter values from which combinations were derived and evaluated for each model are presented in Table \ref{tab:hyper-tuning}. 

%In the case of XGBoost, for example, this included modulation of learning rate $\eta$, $\ell_{1}$ regularization, number of ensemble tree estimators, and maximum tree depth to minimize overfitting while maximizing predictive performance. For purposes of concise presentation, we mainly present figures generated from the tenth fold, which forms a validation set for discussion (e.g. Figure \ref{fig:grs_average_performances}, Figure \ref{fig:results_regression_grs}, Figure \ref{fig:results_regression_maxi}, etc.). Beyond these, there are some results shown like the receiver operator characteristic (ROC) curves in Figure \ref{fig:cm_and_roc_binary} that depict averaged results across repetitions and folds to give an aggregated understanding of referenced concepts that takes inter-fold variance into account.% The remainder of plots for all other models and folds are made available online in a supplementary figure set. 

\begin{table}\label{tab:hyper-tuning}
    \caption{Feature spaces for model hyperparameter tuning}
    \label{tab:pairwise}
    \centering

    \begin{tabular}{llll}
\toprule
 & Decision Tree & Random Forest & Extra Trees \\
\texttt{min\_samples\_leaf} & \{1,3\} & \{1,3\} & \{1,3\} \\ 
\texttt{min\_samples\_split} & \{2,4,6,8\} & \{2,4,6,8\} & \{2,4,6,8\} \\ 
\texttt{n\_estimators} & & \{50,100,150, & \{50,100,150, \\ 
& & 200,250,500\} & 200,250,500\} \\ 
\texttt{warm\_start} & & \{\texttt{True},\texttt{False}\} & \\
\midrule
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Selection}\label{sec:feature_selection}

Through feature selection, it is generally important to deal with potential multicollinearity by calculating Variance Inflation Factors (VIF) and removing features with VIF values $\gtrsim5$ \citep{kline1998principles, Sheather2008-mc}. However, we have chosen not to remove potentially collinear features prior to regression for the following reasons: first, the tree based models like random forest that we focus on are by design robust from the effects of multicollinearity \citep{Strobl2008,2021arXiv211102513C}. Second, since multicollinearity only affects the estimated coefficients of linear models, but not their predictive ability, applying a linear model to potentially collinear data is perfectly reasonable in our case, as we are using the linear model solely as a baseline against which we will compare the predictive capabilities of the more complicated random forests model; i.e., as we are applying the linear model, we are not interested in its components \citep{multicollinearityclass,multicollinearityregression}. We will, however, revisit multicollinearity when we interpret feature importances in Section \ref{sec:results}.

\section{Results} \label{sec:results}

\subsection{Regression}

As demonstrated in Figure \ref{fig:grs_average_performances}, on average our tree-based models outperform linear regression in every regression case, regardless of source or input feature type. Interestingly, as shown in Figure \ref{fig:results_regression_maxi}, linear regression also seriously struggles to correctly assign $0$ values to observations lacking QPOs for both processed and rebinned energy spectra input data, a problem not faced by the other models (except random forest with rebinned energy spectra to a lesser degree). Furthermore, linear regression always has higher dispersion in the relationship between actual and predicted QPO frequency. Yet, despite their unified superiority versus linear regression, the machine learning models do differ significantly within fold amongst themselves, as shown in Figure \ref{fig:grs_average_performances}, Figure \ref{fig:results_regression_grs}, and Figure \ref{fig:results_regression_maxi}. Specifically, although decision tree provides a notable improvement in dispersion between true and predicted values, as well as a slope between these closer to unity, it is by far bested by random forest, and extra trees. Two additional interesting divergences in model performance occur between the sources, as well as between their input types. Regarding the former, all models trained and evaluated on GRS 1915+105 data have more overall dispersion and slopes tending further away from unity in their mapping between true and predicted frequency when compared to the same models for MAXI J1535-571 QPOs with processed input features. This can be clearly seen when comparing Figure \ref{fig:results_regression_grs} versus the top row in Figure \ref{fig:results_regression_maxi}. The superior performance of the algorithms on MAXI J1535-571 are surprising for several reasons: first, with GRS 1915+105 the models never face the problem of false negatives or false positives because there are no QPO-absent data in this set. In contrast, MAXI J1535-571 observations are of varying composition, imbalanced in favor of QPO absence. Second, GRS 1915+105 has around two times more total observations, and around six times more observations with QPOs than MAXI J1535-571; in most cases training models on more data leads to corresponding increases in accuracy \citep{kalinin2020handbook,brefeld2020machine}. However, this assumption may not hold in instances like this, where models are being tested on different objects, as there may exist fundamentally stronger/more pronounced associations between spectral and QPO in one of the systems. The most likely reason for the inferior performance on GRS 1915+105 QPOs is that the underlying relationships between the input and output QPO features are likely more convoluted for GRS 1915+105, which is understandable given GRS 1915+105 has long been known to have complex variability states, and is in fact a bit of an oddball among black-hole systems. Additionally, potential confusion could arise because the models fitted on fundamental QPOs only in GRS 1915+105 intentionally lack the freedom to predict aspects about harmonics, which could lead to these models to potentially confuse signals for harmonics with fundamentals (this is an unexpected insight from our initial decision to only predict for the fundamental in GRS 1915+105 in an effort to explore how the models behave with simpler output space). Finally, to evaluate the performance of the multioutput aspect of the regression, we carry out pairwise nonparametric  two-sided goodness-of-fit Kolmogorov-Smirnov (KS) tests on permutations of QPO parameter residual arrays \citep{KS-test,KS-test-review}, and fail in all instances to reject the hypothesis that any pair of distributions of residual arrays between actual and predicted QPO parameters are not drawn from the same distribution ($p>0.76$ for all GRS 1915+105 and $p>0.99$ for all MAXI J1535-571 residual pair permutations, regardless of input type). This shows that the the models do not favor any particular QPO parameter in their regression and instead regress for each with statistically insignificant differences in accuracy (i.e. accuracy is not different for QPO features, both for the fundamental, as well as the harmonic when present). As for the second interesting divergence in model performance (by input type), surprisingly there is a pronounced difference in model performance when these regression models are trained on processed features as opposed to rebinned energy spectra: in all model cases, dispersion and slope both drastically worsen when models rely on the rebinned energy spectra directly. This is shown for MAXI J1535-571 regression between the top and bottom rows of Figure \ref{fig:results_regression_maxi} demonstrates that although the models could hypothetically learn some lower level representation of the concepts of hardness, overall net count rate, etc. from the data and not require the engineered features, with the amount of data provided  engineered features provide significant additional insight for the models to base decisions on that, exceeding what is provided by energy spectra alone. This would be an interesting idea to investigate with deep learning methods, which would far exceed these classical models' ability to learn abstractions in the data through automated feature extraction \citep{nadeauandbengio}. 

\subsection{Classification}

At least for MAXI J1535-571, binary classification of QPO absence/presence appears to be a fairly trivial task, as shown by the confusion matrices of the first repetition tenth folds in Figure \ref{fig:cm_and_roc_binary}. Additionally, as Figure \ref{fig:cm_and_roc_binary} also shows, our logistic regression classifier corollary to linear regression performs just as well as random forest in terms of accuracy and other classification metrics when trained on processed input data, with negligible difference for rebinned energy spectra as well. This is corroborated by the corresponding ROC curves also shown in Figure \ref{fig:cm_and_roc_binary}. The ROC curves show how a model has optimized between specificity (on the abscissa) and recall (also known as sensitivity; on the ordinate), with the ideal model displaying an ROC curve enclosing an area under curve (AUC) of $1$ \citep{bruce2017practical}. The curves in Figure \ref{fig:cm_and_roc_binary} represent the average ROC and AUC values with $1\pm\sigma$ deviations across all folds and repetitions evaluated. Both logistic regression and random forest decrease in average AUC when trained on rebinned energy spectra, but the decrease is most dramatic for logistic regression. We also present multiclass classification results for multinomial logistic regression and random forest based on processed and rebinned energy spectra input data in Figure \ref{fig:cm_multiclass}. In the case of processed input data, random forest clearly outperforms logistic regression, but both models actually experience noted decreases in accuracy when tasked with predicting multiple outputs corresponding to the actual number of QPOs in a MAXI J1535-571 observation based on rebinned energy spectra input. In fact, in the case of energy spectra inputs, random forest actually performs worse than logistic regression. Overall, the decreased performance of both models here is likely do to the class imbalance in the data set (as mentioned in Section \ref{sec:data_analysis}), which gives the models very few single QPO observations to use as training data per round. 

\begin{figure}
    \centering
\includegraphicswidth=0.29\textwidth{figures/figure_4/GRSmodelscomparisons.pdf}
\includegraphicswidth=0.29\textwidth{figures/figure_4/MAXIfeaturemodelscomparisons.pdf}
    \caption{Gaussian kernel density estimate violin plot representations of aggregated median absolute error for each tested model across $k=10$ validation folds repeated $r=5$ times on GRS 1915+105 (feature input) data (top) and MAXI J1535-571 (feature input) data (bottom). The abbreviations DT, RF, and ET stand for the decision tree, random forest, and extra tree models, respectively. As further discussed in Section \ref{sec:results}, linear regression is outperformed by the classical machine learning models models across folds for each repitition round. Furthermore, the two ensemble tree based models clearly outperform the single decision tree model, which is to be expected.}
    \label{fig:grs_average_performances}
\end{figure}
%\begin{equation}
%    \mathrm{F}_{\beta} = %(1+\beta^2)\cdot\frac{\mathrm{precision}\cdo%t\mathrm{recall}}{(\beta^2\cdot\mathrm{preci%sion})+\mathrm{recall}}
%\end{equation}
\begin{figure*}
    \centering
\includegraphicswidth=0.32\textwidth{figures/figure_5/80701015402fig5pdsdata.pdf}
\includegraphicswidth=0.32\textwidth{figures/figure_5/50703012801fig5pdsdata.pdf}
\includegraphicswidth=0.32\textwidth{figures/figure_5/50703012401fig5pdsdata.pdf}
\includegraphicswidth=0.32\textwidth{figures/figure_5/80701015402fig5pdsmodel.pdf}
\includegraphicswidth=0.32\textwidth{figures/figure_5/50703012801fig5pdsmodel.pdf}
\includegraphicswidth=0.32\textwidth{figures/figure_5/50703012401fig5pdsmodel.pdf}
    \caption{Example PDS with over plotted QPO predictions for the GRS 1915+105 observations 80701-01-54-02, 50703-01-28-01, 50703-01-24-01 ordered by column left to right from least (best-fitting) median to greatest (worst) Pythagorean sum of normalized errors on the three predicted QPO Lorentzian parameters (with corresponding models alone in bottom row). Note that the seemingly diminished height of the predicted QPOs is actually a consequence of how they were determined in the processing procedure, and in the case of the best observation 80701-01-54-02, the amplitude only differs by less than $0.3\%$ from the ``true'' amplitude value it was predicting, as the derived amplitudes had reduced amplitudes originally.}
    \label{fig:example_pds}
\end{figure*}

\begin{figure*}
    \centering
\includegraphicswidth=0.24\textwidth{figures/figure_6/frequencyresultsregressionLinearRegression.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_6/frequencyresultsregressionDecisionTreeRegressor.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_6/frequencyresultsregressionRandomForestRegressor.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_6/frequencyresultsregressionExtraTreesRegressor.pdf}
    \caption{A results regression plot for all QPOs predicted from the test set for the source GRS 1915+105 as returned (from left to right) by linear regression, decision tree, random forest, and extra trees. The best models$-$random forest and extra trees$-$both minimize dispersion between true and predicted values (as quantified by $r^2$), while simultaneously producing the most 1:1 relationships between them (as quantified by best fit slope).}
    \label{fig:results_regression_grs}
\end{figure*}

\begin{figure*}
    \centering
\includegraphicswidth=0.19\textwidth{figures/figure_7/LinearMAXI_J1535571spectrum=Falseresults_regressionfold=9.pdf}
\includegraphicswidth=0.19\textwidth{figures/figure_7/DTMAXI_J1535571spectrum=Falseresults_regressionfold=9.pdf}
\includegraphicswidth=0.19\textwidth{figures/figure_7/RFMAXI_J1535571spectrum=Falseresults_regressionfold=9.pdf}
\includegraphicswidth=0.19\textwidth{figures/figure_7/ETMAXI_J1535571spectrum=Falseresults_regressionfold=9.pdf}\\
     %% ALWAYS PUT SPECTRUM=TRUE ON BOTTOM BY ROW! %% 
\includegraphicswidth=0.19\textwidth{figures/figure_7/LinearMAXI_J1535571spectrum=Trueresults_regressionfold=9.pdf}
\includegraphicswidth=0.19\textwidth{figures/figure_7/DTMAXI_J1535571spectrum=Trueresults_regressionfold=9.pdf}
\includegraphicswidth=0.19\textwidth{figures/figure_7/RFMAXI_J1535571spectrum=Trueresults_regressionfold=9.pdf}
\includegraphicswidth=0.19\textwidth{figures/figure_7/ETMAXI_J1535571spectrum=Trueresults_regressionfold=9.pdf}
    \caption{Same as Figure \ref{fig:results_regression_grs}, except for MAXI J1535-571 observations. Note the increased dispersion and much less 1:1 relationships between true and predicted values for every model in the lower row (rebinned energy spectra as inputs) compared to the upper row (processed feature inputs) and corresponding bottom row. The lesser number of points in these plots stems from both the smaller sample size of MAXI J1535-571 observations, as well as the clustering of values correctly predicted as zeros at the point $(0,0)$ where points cannot be seen individually in this plot). }
    \label{fig:results_regression_maxi}
\end{figure*}

\begin{figure*}
    \centering
\includegraphicswidth=0.24\textwidth{figures/figure_8/LogisticRegressionMAXI_J1535571spectrum=Falsemulti=Falsefold=9confusion_matrix.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/LogisticRegressionMAXI_J1535571spectrum=Falsemulti=Falsefold=9ROC.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/RandomForestMAXI_J1535571spectrum=Falsemulti=Falsefold=9confusion_matrix.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/RandomForestMAXI_J1535571spectrum=Falsemulti=Falsefold=9ROC.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/LogisticRegressionMAXI_J1535571spectrum=Truemulti=Falsefold=9confusion_matrix.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/LogisticRegressionMAXI_J1535571spectrum=Truemulti=Falsefold=9ROC.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/RandomForestMAXI_J1535571spectrum=Truemulti=Falsefold=9confusion_matrix.pdf}
\includegraphicswidth=0.24\textwidth{figures/figure_8/RandomForestMAXI_J1535571spectrum=Truemulti=Falsefold=9ROC.pdf}
    \caption{Confusion matrices and ROC Curves with labeled AUC values for MAXI J1535-571 binary classification cases. The left pairs correspond to logistic regression, whereas the right correspond to random forest. The confusion matrices are taken from the first tenth fold, whereas the ROC curves are averaged across all folds with $\pm1\sigma$ deviations denoted by the grey regions. The superior performance of the models working from processed inputs in the top row compared to their rebinned energy spectra input analogues in the bottom row is intriguing and discussed in more detail in Section \ref{sec:results}.}
    \label{fig:cm_and_roc_binary}
\end{figure*}

\begin{figure*}
    \centering
\includegraphicswidth=0.3\textwidth{figures/figure_8/LogisticRegressionMAXI_J1535571spectrum=Falsemulti=Truefold=9confusion_matrix.pdf}
\includegraphicswidth=0.3\textwidth{figures/figure_8/RandomForestMAXI_J1535571spectrum=Falsemulti=Truefold=9confusion_matrix.pdf}
    \\
\includegraphicswidth=0.3\textwidth{figures/figure_8/LogisticRegressionMAXI_J1535571spectrum=Truemulti=Truefold=9confusion_matrix.pdf}
\includegraphicswidth=0.3\textwidth{figures/figure_8/RandomForestMAXI_J1535571spectrum=Truemulti=Truefold=9confusion_matrix.pdf}
    \caption{Confusion matrices for multiclass MAXI J1535-571 output, where the left column corresponds to logistic regression, the right column to random forest, the top row to processed input features, and the bottom row to rebinned energy spectra input features. Although only the accuracy of logistic regression decreases from binary to multinomial classification based on processed \texttt{XSPEC} input features, both models are significantly more inaccurate for the multinomial case based on energy spectra inputs compared to either binary case in Figure \ref{fig:cm_and_roc_binary}.}
    \label{fig:cm_multiclass}
\end{figure*}

\section{Discussion}\label{sec:discussion}

Now that we have demonstrated QPOs properties can be predicted—and in the following section show how features useful to these predictions can be analyzed—on the sources MAXI J1535-571 and GRS 1915+105 individually, we propose the next step would be to apply these methods in a future work on source-heterogeneous input data, a capability we intentionally incorporate into our QPOML library. To achieve this, it would be beneficial to construct a large standardized database of QPO and spectral data with a scope \textit{à la} \cite{blackcat}, for which the wealth of \textit{RXTE} observations will prove invaluable. Additionally, while increasing source sample size like this, it would also be fruitful to include neutron star LFQPOs and kHz QPOs in a followup study to generalize between sources, because unlike BH XRBs, NS XRBs are predominantly persistent and have significantly more observations with QPOs in archival \textbf{\textit{RXTE}} data in general \citep{mendez4U-1608,Migliari2003,belloniMendez2005,cenX-3QPOs}. That being said, the likely trade-off of using \textbf{\textit{RXTE}} data for these sources is that these QPOs will be predicted based on engineered XSPEC features instead of raw spectra given gain drift, as was the case with our analysis of GRS 1915+105 versus MAXI J1535-571. Another potential avenue for extending this work would involve exploring new input features to associate with QPOs, such as black hole spin, mass, inclination, jet properties, and QPO phase lags, and tracking the importance of variable features throughout outburst and accretion states to see if they evolve in tandem. Including scattering fraction as an input parameter promises interesting results as well, because QPO frequency and scattering fraction exhibit a correlation for sources like MAXI J1535-571 but an anti-correlation for other objects including GX 339-4, H1743-322 and XTE J1650-550 \citep{gargEnergyDependence}. Finally, how these non-parametric machine learning models interact with the polynomial/exponential versus sigmoidal relationship between frequency and power-law index for some black holes versus neutron stars \citep{titarchukSigmoidExp}, as well as how well models trained on distinct outbursts of certain objects perform for outbursts withheld from their training, would both also be of interest if these models are applied on samples that differ not only by source, but also by source type (BH or NS). Now, we turn to discussing feature importances in Section \ref{sec:feature_imps} and statistically compare the models we used throughout this work in Section \ref{sec:model_comparison}. 

\subsection{Feature Importances and Interpretation}\label{sec:feature_imps}

Feature importances refer to the relative attributed weights a model gives to different input features \citep{Saarela2021ComparisonOF}. In other words, they are measures for how helpful different features are for the model in making correct predictions, regardless of whether these predicted values are categorical or real-valued \citep{generalPermutations}. Before we discuss these, however, we will briefly describe our efforts to ensure the interpretability of our machine learning models. Interpretability is defined parsimoniously by \cite{tim-miller-interpretability} as the degree to which a human can understand the cause of a decision. Since most of our models are intrinsically complex (except for linear and logistic regression and decision trees), we seek \textit{post hoc} interpretability through feature importances \citep{post-hoc}. These values should not be interpreted as substitutes for other e.g. parametric importances, because they seek to explain how a machine learning model learns and interacts with its data. However, we believe that properly calculated feature importances may offer alternative helpful insight about the origins of QPOs, and we therefore take steps to avoid common pitfalls associated with these measures. For example, although it is common to discuss default impurity-based feature importances, this approach is flawed because it is both biased towards high-cardinality numerical input features, as well as computed on training set statistics, which means it may not accurately generalize to held-out data \citep{scikit-learn}. Additionally, although permutation importances are commonly put forward as a superior alternative, these suffer from multicollinearity, as in the process of permutating single features, an impactful feature could be erroneously ascribed as having little-to-no effect on model performance if it has high correlation with another feature \citep{Strobl2007,Nicodemus2010,hooker2019}. Therefore, we chose to to determine feature importances with the contemporary \texttt{TreeSHAP} algorithm as implemented in the Python package \texttt{shap} by \cite{SHAP2017}. This model extends game theoretic coalitional Shapley values to calculate SHapley Additive exPlanations (SHAP) in the presence of multicollinearity by incorporating conditional expected predictions \citep{shapleyvaluesoriginal,SHAP2017,molnar2022}. As hinted earlier and detailed in \cite{SHAP2017} and \cite{molnar2022}, an additional benefit of using tree based models is that through tree traversal and dynamic programming the computational cost for computing SHAP values is brought down from exponential time $\mathcal{O}(2^n)$ to   $\mathcal{O}(n^2)$ polynomial time. We calculate feature importances shown in Section \ref{sec:results} for each model $f$ by treating the model from the tenth fold in the first repetition as if they were taken from the test set, and averaging their $\phi_i(f,x)$ from Equation \ref{eq:shapley}, which represents the weighted average of differences in model performance when a feature $x$ out of $M$ simplified input features is present versus absent for all subsets $z'\subseteq x'$.

\begin{equation}\label{eq:shapley}
    \phi_i(f,x) = \sum_{z' \subseteq x'} \frac{|z'|!(M - |z'| -1)!}{M!} \left [ f_x(z') - f_x(z' \setminus i) \right ]
\end{equation}

\begin{figure*}
    \centering
\includegraphicswidth=0.29\textwidth{figures/figure_9/ETGRS_1915+905spectrum=Falsefeature_importancesfold=9.pdf}
\includegraphicswidth=0.29\textwidth{figures/figure_9/ETMAXI_J1535571spectrum=Falsefeature_importancesfold=9.pdf}
\includegraphicswidth=0.29\textwidth{figures/figure_9/ETMAXI_J1535571spectrum=Truefeature_importancesfold=9.pdf}
    \caption{Tree-SHAP calculated average of absolute value SHAP feature importances for the most accurate predictive regression models for GRS 1915+105 engineered inputs (left, extra trees), MAXI J1535-571 engineered inputs (middle, extra trees), and MAXI J1535-571 energy spectra inputs (right, extra trees). The features denoted $A-F$ correspond to net count rate, hardness ratio, asymptotic power-law photon index, \texttt{nthcomp} normalization, inner-disk temperature, and \texttt{diskbb} normalization features, respectively. The error bars on each importance correspond to 99$\%$ confidence intervals on mean importances, the dashed line the median importance of all features, and the dotted line the mean of the same. Features corresponding to hard channel count rates are significantly more important than the median and mean feature importance, which is likely related to the higher energy origin of QPOs. An interesting difference between these plots and that for GRS 1915+105 in Figure \ref{fig:feature_importances_reg} is that Extra Trees primarily weights \texttt{diskbb} normalization for MAXI J1535-571 regression but splits primary importance for GRS 1915+105 between the net count rate and hardness ratio engineered inputs.}
    \label{fig:feature_importances_reg}
\end{figure*}

\begin{figure*}
    \centering
\includegraphicswidth=0.29\textwidth{figures/figure_9/RandomForestMAXI_J1535571spectrum=Falsemulti=Falsefold=9FeatureImportances.pdf}
\includegraphicswidth=0.29\textwidth{figures/figure_9/RandomForestMAXI_J1535571spectrum=Truemulti=Falsefold=9FeatureImportances.pdf}
    \caption{Similar to Figure \ref{fig:feature_importances_reg}, except for the best classification models for MAXI J1535-571 binary output based on engineered inputs (left, random forest), and energy spectral inputs (right, random forest). As seen for regression, hard energy channels similarly dominant feature importances for energy spectra input, yet, while \texttt{diskbb} normalization is still the most important processed feature for classification, more importance is attached here to net count rate and \texttt{nthcomp} normalization here than for regression on MAXI J1535-571.}
    \label{fig:feature_importances_class}
\end{figure*}

One of the most important things shown by Figure \ref{fig:feature_importances_reg} and \ref{fig:feature_importances_class} is that there are significant interesting differences between the feature importances attributed to the processed features for GRS 1915+105 and MAXI 1535-571, which may be related to the nuances of the process driving QPOs in these systems. For example, in GRS 1905+105, net count rate and hardness ratio are clearly the most important features, after which importance falls precipitously and remains uniformly modest for the rest, with this proportional decrease ranging from a factor of three for \texttt{nthcomp} asymptotic power law to six for \texttt{nthcomp} and \texttt{diskbb} normalization. Because we have used SHAP values for importance, we can rule out the un-importance of these features stemming from multicollinearity or training set artifacts, which means they could \textit{potentially} be related to curious physical related conditions. However, there is no ambiguity about the importance of net count rate and hardness, because an XRB outburst's q-shaped state-evolution in the hardness-intensity diagrams (HIDs) is known to also be indicative of changes in timing (e.g., QPO) properties as tracked in HIDs \citep{motta2015LotsofQPOs,mottaquickreview}. This is also in agreement with the findings of Figure 2 of \cite{evolvingPropertiesGRSCorona}, in which the QPO frequency of GRS 1915+105 is shown to vary with a somewhat inverse relationship with hardness ratio across mostly horizontal and vertical gradients in inner disk temperature and power law index, respectively. %Mention a motta paper for HID information and KNN style HID association. 

In contrast to GRS 1915+105, the feature importances for both the best regression and classification models on processed MAXI J1535-571 input features favor a single feature above all others: \texttt{diskbb} normalization (although in the case of classification, net count rate and \texttt{nthcomp} normalization are still significant for MAXI J1535-571). This quantity (ignoring relativistic and plasma corrections) approximately corresponds to the projected area of the inner-disk on the sky: $N_{\mathrm{disk}}=(\frac{R_{in}}{D_{10}})^2\cos(\theta)$, where $R_{in}$ is the apparent inner disk radius in km, $D_{10}$ is the distance to the source in 10 kpc units, and $\theta$ the angle of the disk \citep{XSPEC1999}. This prominent importance is intriguing because it implies a dependence between QPO presence and frequency on \texttt{diskbb} normalization and therefore inner disk radius. This is corroborated by \cite{gargEnergyDependence}, who find that QPO frequency correlates significantly with the inner disk radius for MAXI J1535-571 in data provided by \textit{AstroSat} according to the power law relationship $v_{\mathrm{QPO}}\propto \dot{M} R_{in}^p$, where $\dot{M}$ is mass-accretion rate \citep{astrosat}. However, \citep{gargEnergyDependence} do not find a clear relationship between \texttt{diskbb} normalization and QPO frequencies in the $\sim1.6-2.8$ Hz range. Overall, the similarity in feature importances for engineered features for regression and classification in MAXI J1535-571 shows that the same features that are  important in determining the parameterizations of QPOs are those important in determining their presence vs absence. Regarding the feature importances derived from the energy spectra, the highest energy channels are the most important for both regression and classification, with the five most important channel counts rates for each coming from the equivalent $[9.5-10), [9.0-9.5), [8.5-9.0), [8.0-8.5)$ and $[7.5-8.0)$ keV channels for regression and $[9.0-9.5), [9.5-10.0), [8.5-9.0), [8.0-8.5)$ and $[3.0-3.5)$ keV channels for classification. Notably, for both classification and regression only hard channels $\geq 3$ keV have importances significantly greater than the mean and median importances for all features in their respective sets at the $99\%$ confidence level. The fact that the high-energy spectral data is most informative of the QPOs is interesting and we speculate that this may be related to the fact that QPOs manifest more prominently at higher energies above the disk’s peak temperature. A broader perspective which generalizes these relationships to other BH systems is of high interest, but outside the scope of this work. Consequently, we are currently working on a comprehensive follow-up work, in which we will evaluate these models on data identically reprocessed for numerous black holes and neutron stars simultaneously. One additional difference between this preliminary work and that prospective one will be full inclusion of all LF QPO features for all sources (such as GRS 1915+105), because although focusing on the dominant frequency for QPOs in GRS 1915+105 served our purposes here, this would be a limitation in the future because such focus would not make it clear whether these trained forest methods would predict many false positives and false negatives for sources similar to GRS1915+105 that do include QBO-absent data, yet perform well nonetheless.

\subsection{Statistical Model Comparison} \label{sec:model_comparison}

As mentioned in Section \ref{sec:methods}, we included an ordinary least squares model as a benchmark for their utilization. As Figure \ref{fig:grs_average_performances}, Figure \ref{fig:results_regression_grs}, and \ref{fig:results_regression_maxi} demonstrate, each of our models outperform linear regression. In order to assess the significance of the improvements, we employ the \cite{nadeauandbengio} formulation of the frequentist Diebold-Mariano corrected paired t-test \citep{dieboldandmariano}, 

% Nadeu and Bengio: https://papers.nips.cc/paper/1661-inference-for-the-generalization-error.pdf

% Diebold-Mariano: http://www.est.uc3m.es/esp/nueva_docencia/comp_col_get/lade/tecnicas_prediccion/Practicas0708/Comparing%20Predictive%20Accuracy%20(Dielbold).pdf

\begin{equation}\label{eq:freq}
    t=\frac{\frac{1}{k \cdot r}\sum_{i=1}^{k}\sum_{j=1}^{r}x_{ij}}
{\sqrt{(\frac{1}{k \cdot r}+\frac{n_{\mathrm{test}}}{n_{\mathrm{train}}})\hat{\sigma}^2}}
\end{equation}

\noindent where $k=10$ and represents the number of k-fold validation folds, $r=10$ and equals the number of times we repeated the $k$-fold procedure, $x$ is the performance difference between two models, and $\hat{\sigma}^2$
represents the variance of these differences \citep{scikit-learn}. It is necessary to correct the $t-$values in this manner because the performances of the models are correlated with each fold upon which they are tested, as some folds may make it harder for one of, or all of, the models to generalize, whereas others make it easier, and thus the collective performance of the models varies. The results of these pairwise tests for all permutations of two models on both sources is shown in Table \ref{tab:pairwise}. 

We additionally implement the Bayesian \cite{benavoli} approach, which allows us to calculate the \textit{probability} that a given model is better than another, using the Student distribution formulated in Equation (\ref{eq:bay}): 

% Benavoli et al: http://www.jmlr.org/papers/volume18/16-305/16-305.pdf

\begin{equation}\label{eq:bay}
    \mathrm{St}(\mu;n-1,\overline{x},(\frac{1}{n}+\frac{n_{\mathrm{test}}}{n_{\mathrm{train}}}) %St = standard part function
\hat{\sigma}^2)
\end{equation}

\noindent where $n$ is the total number of samples, $\Bar{x}$ is the mean score difference, and $\hat{\sigma}^2$ is the \cite{nadeauandbengio} corrected variance in differences \citep{scikit-learn}. Both sets of these pairwise tests are also shown in Table \ref{tab:pairwise}. 

Based on these tests, it is clear that extra trees significantly out performs all other models, and interestingly, that each model that follows it in decreasing order of performance is significantly better than the remaining models following it, confirming the findings in Figure \ref{fig:grs_average_performances}. In fact, in all cases of regression (), the order of model performances is extra trees, random forest, decision tree, and finally, linear regression. This result is expected, with decision trees being more accurate than linear regression (because the former can leverage non-linear relationships between input features and QPOs), as well as for random forest to outperform individual decision trees (because random forests are ensemble aggregations of decision tree forests). The similar yet superior performance of extra trees in comparison to random forest is notable but not striking \citep{mathew2022optimized}, yet this improvement should be considered with the additional size of an extra trees model compared to a trained random forest counterpart (this difference ranges from larger in terms of leaf count) \citep{extratrees}. Nevertheless, based on these findings it is clear that these classical machine learning models have been able to fairly accurately optimize for individual sources. However, although extra trees may perform best in these individual source scenarios, it remains yet to be seen whether these classical models will be generalizable for accurate cross-source analyses (as proposed earlier) or if other models like neural networks will be required \citep{NIPS2017_10ce03a1}. Although it may seem reasonable to combine data from these two sources and evaluate the predictive performance of these models in such a source-heterogeneous space, this would not be appropriate because any the resultant feature importances would not communicate whether or not the input engineered or raw spectral features are being leveraged for intuition into the physical state of the objects, or if their importances just reflect the models picking up on artifacts from the data generation procedure. In other words, this could be considered a form of data leakage, considering differing instrumental sensitivities, QPOs identification methods for each source, etc. \citep{Hannun2021MeasuringDL,2022arXiv220903345Y}. Hence, this provides additional motivation for follow-up, in which energy and timing spectra from a single instrument are reprocessed in an identical manner for multiple objects to prevent instrumental artifacts from contaminating the findings potentially recoverable from such a source-heterogeneous data-set. 

%\subsection{Frequency Bias}\label{sec:freq_bias}

\begin{figure*}
    \centering
\includegraphicswidth=0.6\textwidth{figures/figure_10/GRS_qpo_pairplot.pdf}
    \caption{A pairplot displaying the pairwise relationships between engineered input and Lorentzian QPO output paramters for all GRS 1915+105 data. The letters in $A-F$ correspond to the net count rate, hardness ratio, asymptotic power-law photon index, \texttt{nthcomp} normalization, inner-disk temperature, and \texttt{diskbb} normalization features, respectively.}
    \label{fig:pairplot} 
\end{figure*}

\section{Conclusion} \label{sec:conclusion}

In this paper we have advanced novel approaches utilizing machine learning algorithms to link energy spectral properties (as both rebinned raw energy spectra and alternatively via engineered features derived from spectral fits) with the presence and properties of QPOs prominent in power-density spectra of two low-mass X-ray binary black hole systems. Specifically, we tested a selection of tree-based classical machine learning models using engineered features derived from energy spectra to predict QPO properties for fundamental QPOs in the black hole GRS 1915+105, and such derived features as well as raw rebinned energy spectra to characterize fundamental and harmonic QPOs in the black hole MAXI J1535-571. Additionally, we trained classification algorithms on the same data to predict the presence/absence of QPOs, as well as the multiclass QPO state of MAXI J1535-571 observations. We compared the performance of the machine learning models against each other, and found extra trees to perform best in all regression situations for both sources. Additionally, we compared every model against simplistic linear (regression) and logistic (classification) models as well, finding the machine learning models outperformed their linear counterpart in all regression cases, with linear regression notably struggling to correctly identify observations lacking QPOs. The main findings from this study are:

\begin{enumerate}
    \item All tested regression models yielded significantly better results on MAXI 1535-571 versus GRS 1915+105 data, despite the latter having 6x more data with QPOs and no issue with QPO absent observations. We attributed this to the multitude of unusual variability classes unique to GRS 1915+105 among \cite{MLStatesofGRS}.
    \item Kolmogorov-Smirnov tests on permutations of QPO parameter residuals showed that the best fitting regression model, Extra Trees, does not favor any particular QPO parameter and instead predicts for all with equal accuracy, including those for harmonics.
    \item Using rebinned raw spectral data as opposed to \texttt{XSPEC} derived features resulted in significantly worse performance for regression, binary classification, and multiclass classification on MAXI J1535-571 observations. 
    \item To enhance computational efficiency and ensure importance credibility, we calculated \texttt{TreeShap} feature importances immune to multicollinearity and found that for processed input features, extra trees determined the most significant features for GRS 1915+105 to be net count rate and hardness ratio, whereas the same model predicting for MAXI J1535-571 found \texttt{diskbb} normalization most important, which suggests a dependence on physical inner disk radius in this case. 
    \item We found almost all the rebinned channels which are the most important in determining the parameterizations of QPOs in regression are also those that are most important in determining their presence versus absence in classifying MAXI J1535-571 energy spectral data. Furthermore, for energy spectra, we found hard channels are the most important for both regression and classification, which aligns with the understanding of higher energy QPO manifestation above peak disk temperatures
    \item We have proposed future applications of these methods that range from extending the input feature space they are tested on (e.g. scattering fraction and inclination) to moving from single source to source/source-type heterogeneous samples to achieve our original goal of inter-object generalizations since in this paper we have introduced and laid the foundation for these methods on individual objects. 
\end{enumerate}

Finally, we based our work on our \texttt{QPOML} Python library, from input and output matrix construction and preprocessing, to hyperparameter tuning, model evaluation, and plot generation, which were all conveniently streamlined for application and both (i) executed as ``under-the-hood'' as possible while remaining user accessible; and (ii) easily extendable to any number of QPOs and any number of scalar observation features for any number of observations from any number of sources. We preview QPOML in Appendix A, and provided source code as well as documentation for it on GitHub. 

%Looking towards the future, we are currently working on a follow up piece to this paper in which we are evaluating an array of inter-source generalizing models for global-scale insights on the relationship between energy properties of LMXRBs and their QPOs, similiar in source comprehensiveness to \cite{Dunn2010}. As part of this we are working on assembling a large standardized database of QPO and spectral data with a scope a la \cite{blackcat} in immediately machine readable tabular format and we welcome collaborators in this effort. We also envision further extending this work in many other directions, and here summarize a selection of these potential areas for future related exploration. First, although this work focused on low frequency QPOs (LFQPOs) in black hole LMXRBs, we believe in addition to increasing source sample size to generalize between sources, that a very smart next step would be to include neutron star LFQPOs or kHz QPOs in a followup study, because unlike BH LMXRBs, NS LMXRBs are predominately persistent and have Xs of X-seconds of QPOs in archival \textbf{\textit{RXTE}} data. Beyond neutron stars high mass X-ray binaries (HMXRBs) may also be of interest. Another potential avenue for extending this work would involve exploration of new input features to associate with QPOs, such as black hole spin, mass, inclination, jet properties, and QPO phase lags, which will all will likely be very useful in distinguishing QPO properties in mixed source samples. The inclusion of scattering would likely lead to interesting results as well, especially in mixed-source datasets because QPO frequency and scattering fraction has been shown to exhibit a correlation for sources like MAXI J1535-571 but an anti-correlation for other objects including GX 339-4, H1743-322 and XTE J1650-550 \citep{gargEnergyDependence}. Mention polynomial versus sigmoid relationship between frequency and power-law index for neutron stars versus black holes in that \textbf{titachurk and shaposhnikov 2005 paper}

%Potentially an interesting application of Deep Learning to this field would be utilization of deep learning models on source light curves themselves. 

%transfer learning from simulations to application in the wild? 

%In short, we believe that opening QPOs to study with machine learning will not only provide interesting insights that will help us as a community better understand QPOs, but that his will consequently advance understanding of XRBs in general, which is important given the fundamental physics tested/elucidated by them, as well as the role they play in unique role in the formation and evolution of galaxies at large. Overall, we strongly believe that machine learning should not be advanced panacea-style as the ideal method for all situations, and our work does definitely not advocate for machine learning methods to replace traditional observational and or modeling methods. Instead, we hope that in the context of high energy astrophysics, our work opens the door to present and future creative, unique insights. Much like \cite{Licklider1960ManComputerS}, we aim for symbiosis, specifically between traditional and machine learning models to best solve problems like that of the origin of quasi-periodic oscillations. 

\section{Acknowledgements}

M.M. acknowledges support from the research program Athena with project number 184.034.002, which is (partly) financed by the Dutch Research Council (NUD). We also thank Virginia A. Cuneo for a helpful conversation early in this work, Michael Corcoran and Craig Gordon for assistance with some early technical issues. Finally, we thank Travis Austen with help recovering a significant amount of our work from a damaged virtual machine disk, and Brandon Barrios for Windows Subsystem for Linux advice. This work was made possible by the \textit{NICER} and \textit{RXTE} missions, as well as data from the High Energy Astrophysics Science Archive Research Center (HEARSARC) and NASA's Astrophysics Data System Bibliographic Services. 

\section{Data Availability}
The data used for MAXI J1535-571 are available at the \textbf{\textit{NICER}} archive (https://heasarc.gsfc.nasa.gov/docs/nicer/nicer\_archive.html), and those for
GRS 1915+105 belong to their corresponding authors and are available at the following references \cite{GRSDATAPAPER} and \cite{zhangGRS2022}. The software used for energy spectral data analysis can be accessed from the HEASARC website
(https://heasarc.gsfc.nasa.gov/lheasoft/download.html). \\

\noindent \textit{Facilities:} \textit{NICER}, \textit{RXTE}

\noindent \textit{Software:} \textit{Software:} AstroPy \citep{astropy1,astropy2}, Keras \citep{keras}, Matplotlib \citep{Hunter2007}, NumPy \citep{numpy}, Pandas \citep{pandas}, SciencePlots \citep{SciencePlots}, SciPy \citep{Virtanen2020}, scikit-learn \citep{scikit-learn}, and seaborn \citep{Waskom2021}.

%\bibliographystyle{plainnat}
\bibliographystyle{mnras}
\bibliography{bibliography}
\section*{Appendix}
\subsection*{A. QPOML Demonstration} \label{appendix:A}
\newpage
\begin{figure}
    \centering
    
    \begin{minted}[mathescape,gobble=2]{python}
    from sklearn.ensemble import RandomForestRegressor
    import matplotlib.pyplot as plt 
    from qpoml import collection
    
    collec = collection(qpo_path='./QPO.csv', context_path='./Context.csv', approach='regression')
                        
    collec.evaluate(model=RandomForestRegressor(), folds=10, repetitions=5)

    fig, axs = plt.subplots(1, 2, figsize=(8,4))

    collec.plot_results_regression(feature_name='frequency', which=[0], ax = axs[0])
                                   
    collec.plot_feature_importances(style='bar', ax=axs[1])
                                    
    plt.show()
    
    \end{minted}
    \caption{Here we present a brief demonstration of using \texttt{QPOML}. After QPO features have been aggregated and saved in a formatted csv file, the path to this csv file is passed to the \texttt{qpoml.collection} class to initiate a new \texttt{collection} object along with a path to the corresponding similarly formatted context csv file, which contains the input information for every observation, such as processed \texttt{XSPEC} or rebinned energy spectrum features. The program then automatically matches the vectors in input and output space by observation identifier, and then drops observation identifiers to order these matrix vector entries randomly while maintaining index correspondence, while simultaneously preprocessing the values parameter-wise and finally organizing them in the input and output matrices. Then, during evaluation, the provided model is tested across three repeated $k=10$ cross validation runs, where the folds are automatically stratified to QPO presence/absence if this is a detected aspect of the data. Finally, the process of making plots like those in Figure \ref{fig:results_regression_grs} and Figure \ref{fig:feature_importances_reg} post-evaluation is as easy as running the corresponding one line functions in \texttt{QPOML}.}
    \label{fig:QPOML-DEMO}
\end{figure}

\newpage
\subsection*{B. Model Comparison} \label{appendix:B}

\begin{table*}
    \caption{Pairwise fold corrected frequentist and Bayesian statistics for all regression model comparisons discussed in Section \ref{sec:discussion}, where GRS 1915+105 models are only tested on extracted (Processed) features, whereas MAXI J1535-571 models are tested on both Processed, as well as rebinned raw energy spectra features (Spectral). Abbreviations-wise, ET (Extra Trees), RF (Random Forest), and DT (Decision Tree). The $t$ values represent the fold-corrected Student's $t$ values of the differences of the average residual values for each model. These are accompanied by their corresponding $p$ values.}
    \label{tab:pairwise}
    \centering

    \begin{tabular}{lllrlrr}
\toprule
Source (Input Type) & First Model Name & Second Model Name &    t &            p &  \% Chance & \% Chance \\ & & & & & First Better & Second Better \\
\midrule
MAXI J1535-571 (Spectral) & & & & & & \\
&  ET & RF & 0.67 & 0.25 &                  74.74 &                   25.26 \\
&  ET & DT & 0.83 & 0.21 &                  79.60 &                   20.40 \\
&  ET & Linear & 5.73 & 0.00 &                 100.00 &                    0.00 \\
& RF & DT & 0.18 & 0.43 &                  57.12 &                   42.88 \\
& RF & Linear & 5.17 & 0.00 &                 100.00 &                    0.00 \\
& DT & Linear & 5.66 & 0.00 &                 100.00 &                    0.00 \\
MAXI J1535-571 (Processed) & & & & & & \\
&  ET & DT &  0.40 & 3.47e-01 &                  65.45 &                   34.55 \\
&  ET & RF &  0.60 & 2.76e-01 &                  72.59 &                   27.41 \\
&  ET & Linear & 11.21 & 9.35e-12 &                 100.00 &                    0.00 \\
& DT & RF &  0.15 & 4.40e-01 &                  56.00 &                   44.00 \\
& DT & Linear &  8.74 & 1.62e-09 &                 100.00 &                    0.00 \\
& RF & Linear &  9.73 & 1.86e-10 &                 100.00 &                    0.00 \\
GRS 1915+105 (Processed) & & & & & & \\
&  ET & RF &  1.25 & 1.07e-01 &                  89.37 &                   10.63 \\
&  ET & DT &  4.24 & 4.33e-05 &                 100.00 &                    0.00 \\
&  ET & Linear & 11.20 & 4.19e-16 &                 100.00 &                    0.00 \\
& RF & DT &  3.61 & 3.29e-04 &                  99.98 &                    0.02 \\
& RF & Linear &  9.04 & 9.27e-13 &                 100.00 &                    0.00 \\
& DT & Linear &  4.51 & 1.75e-05 &                 100.00 &                    0.00 \\
\bottomrule
\end{tabular}
\end{table*}

\end{document}