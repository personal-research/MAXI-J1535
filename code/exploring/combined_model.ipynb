{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\r\n",
    "\r\n",
    "* One of the issues I noticed with the previous regression model when I was testing on QPO frequencies was although it could predict Frequencies for true QPOs well, the network was very bad at predicting 0 as the frequency for \"qpos\" that don't exist, i.e. it would predict 0.6 for the frequency of a third qpo when that pds only had two qpos. See this plot: \r\n",
    "\r\n",
    "![](https://raw.githubusercontent.com/personal-research/MAXI-J1535/main/archive/images/frequency_regression.png \"Initial Frequency Regression\")\r\n",
    "\r\n",
    "* My proposed solution is this: combine the classification model I designed in ```interim.ipynb``` with the regression model I designed in ```exploring_regression_architectures.ipynb```. Since the classification model could predict (with ~85% accuracy) the number of QPOs, what if I make the model predict the number of QPOs first, and then automatically return 0s for outputs of the \"buffer\" qpos. For example, if the model predicts that there is only one qpo, it will only run regression for one frequency and return the frequency array ```[freq, 0, 0]``` rather than attempting to fit something weird for those second two frequencies when they should be zero. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notes on Models for Combined Classification and Regression\r\n",
    "\r\n",
    "* [machine learning mastery article](https://machinelearningmastery.com/neural-network-models-for-combined-classification-and-regression/)\r\n",
    "\r\n",
    "```python\r\n",
    "# split into input (X) and output (y) variables\r\n",
    "X, y = dataset[:, 1:-1], dataset[:, -1]\r\n",
    "X, y = X.astype('float'), y.astype('float')\r\n",
    "n_features = X.shape[1]\r\n",
    "# encode strings to integer\r\n",
    "y_class = LabelEncoder().fit_transform(y)\r\n",
    "n_class = len(unique(y_class))\r\n",
    "# split data into train and test sets\r\n",
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_class, test_size=0.33, random_state=1)\r\n",
    "# input\r\n",
    "visible = Input(shape=(n_features,))\r\n",
    "hidden1 = Dense(20, activation='relu', kernel_initializer='he_normal')(visible)\r\n",
    "hidden2 = Dense(10, activation='relu', kernel_initializer='he_normal')(hidden1)\r\n",
    "# regression output\r\n",
    "out_reg = Dense(1, activation='linear')(hidden2)\r\n",
    "# classification output\r\n",
    "out_clas = Dense(n_class, activation='softmax')(hidden2)\r\n",
    "# define model\r\n",
    "model = Model(inputs=visible, outputs=[out_reg, out_clas])\r\n",
    "# compile the keras model\r\n",
    "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\r\n",
    "```\r\n",
    "\r\n",
    "![](https://machinelearningmastery.com/wp-content/uploads/2021/02/Plot-of-the-Multi-Output-Model-for-Combine-Regression-and-Classification-Predictions.jpg \"Network Flowchart\")\r\n",
    "\r\n",
    "## Model Merging in Keras\r\n",
    "* [stack exchange article](https://datascience.stackexchange.com/questions/26103/merging-two-different-models-in-keras)\r\n",
    "\r\n",
    "![](https://i.stack.imgur.com/2xIdb.png \"Network Flowchart\")\r\n",
    "\r\n",
    "\r\n",
    "```python\r\n",
    "from keras.layers import Input, Dense\r\n",
    "from keras.models import Model\r\n",
    "from keras.utils import plot_model\r\n",
    "A1 = Input(shape=(30,),name='A1')\r\n",
    "A2 = Dense(8, activation='relu',name='A2')(A1)\r\n",
    "A3 = Dense(30, activation='relu',name='A3')(A2)\r\n",
    "\r\n",
    "B2 = Dense(40, activation='relu',name='B2')(A2)\r\n",
    "B3 = Dense(30, activation='relu',name='B3')(B2)\r\n",
    "\r\n",
    "merged = Model(inputs=[A1],outputs=[A3,B3])\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update\r\n",
    "* I think I'll either use a custom layer or custom activation function. Currently pursuing the latter. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import keras\r\n",
    "from keras import models, layers\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "from keras.layers import Input, Dense\r\n",
    "from keras.models import Model\r\n",
    "from keras.utils import plot_model\r\n",
    "\r\n",
    "np.set_printoptions(suppress=True)\r\n",
    "data_df = pd.read_csv('https://github.com/personal-research/MAXI-J1535/raw/main/data/processed/full.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df = data_df.sample(frac=1) # shuffle the dataframe\r\n",
    "\r\n",
    "def normalize(x):\r\n",
    "    x = (x-min(x))/(max(x)-min(x))\r\n",
    "    return x\r\n",
    "\r\n",
    "# X\r\n",
    "\r\n",
    "arr_names =  ['hardness', 'tins', 'disk_norm', 'gammas', 'nthcomp_norms', 'intensities']\r\n",
    "(hardness, tins, disk_norms, gammas, nthcomp_norms, intensities) = (normalize(np.array(data_df[arr_name])) for arr_name in arr_names)\r\n",
    "\r\n",
    "## Stack x values\r\n",
    "x_vals = np.array([], dtype=np.int64).reshape(0,6) # why are they type int\r\n",
    "\r\n",
    "for a, b, c, d, e, f in zip(hardness, tins, disk_norms, gammas, nthcomp_norms, intensities): \r\n",
    "    new_arr = np.array([float(a), float(b), float(c), float(d), float(e), float(f)])\r\n",
    "    x_vals = np.vstack([x_vals, new_arr])\r\n",
    "\r\n",
    "# Y \r\n",
    "y_vals = np.array([], dtype=np.float32).reshape(0, 3)\r\n",
    "\r\n",
    "freq1s, freq2s, freq3s = (normalize(np.array(data_df[arr_name])) for arr_name in ['first_par1s', 'second_par1s', 'third_par1s'])\r\n",
    "\r\n",
    "for freq1, freq2, freq3 in zip(freq1s, freq2s, freq3s): \r\n",
    "    new_arr = np.array([float(freq1), float(freq2), float(freq3)])\r\n",
    "    y_vals = np.vstack([y_vals, new_arr])\r\n",
    "\r\n",
    "# Train test split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_vals, y_vals, test_size=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# custom function\r\n",
    "from keras import backend as K\r\n",
    "\r\n",
    "def cullBuffers(x, qpo_classes): # qpo_classes needs to be determined before hand\r\n",
    "    new_x = np.array([], dtype=np.float64).reshape(0,3)\r\n",
    "    for x_row, qpo_class in zip(x, qpo_classes): \r\n",
    "        if qpo_class == 0: \r\n",
    "            new_x = np.vstack([new_x, np.zeros(3)])\r\n",
    "\r\n",
    "        elif qpo_class == 1:\r\n",
    "            new_x = np.vstack([new_x, [x_row[0], 0, 0]])\r\n",
    "\r\n",
    "        elif qpo_class == 2: \r\n",
    "            new_x = np.vstack([new_x, [x_row[0], x_row[1], 0]])\r\n",
    "\r\n",
    "        elif qpo_class == 3: \r\n",
    "            new_x = np.vstack([new_x, x_row[0:3]])\r\n",
    "\r\n",
    "    return new_x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(cullBuffers([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.], [4., 4., 4.,]], [0, 1, 2, 3]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "d3c0f98bc337faacb3dc92d86fb3c3ff5c297fcab3c4973bdf073a6202ea54c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}